{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Forest Fire Prediction**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Data cleaning\n",
    "In this section, we will focus on cleaning the raw data to ensure its quality and consistency. Data cleaning is an essential step in the data preprocessing pipeline, as it helps to eliminate errors, handle missing values, and transform the data into a suitable format for analysis.\n",
    "\n",
    "Data handling stages:\n",
    "- Handling Missing Values\n",
    "- Handling Duplicates\n",
    "- Data Conversion\n",
    "- Outliers and Validations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Imports section:** (and warning exception handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note if running on a clean environment, need to install missing modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date as dt\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are creating a new CSV file in each stage to minimize data loss if accrues \n",
    "CSV_NAME = 'fire_history.csv'\n",
    "COLS_RED_CSV = 'fire_history_cols_reduction.csv'\n",
    "MISS_CSV = 'fire_history_miss_values_removed.csv'\n",
    "DUP_CSV = 'fire_history_dup_values_removed.csv'\n",
    "DATA_CONV_CSV = 'fire_history_data_conv.csv'\n",
    "CSV_OUTLIERS = 'fire_history_outliers_removed.csv'\n",
    "\n",
    "FINAL_CSV = 'fire_history_final.csv'\n",
    "\n",
    "COLS = ['UniqueFireIdentifier', 'FireDiscoveryDateTime', 'FireOutDateTime', 'InitialLatitude', 'InitialLongitude', 'POOCounty', 'FireCause']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing unnecessary columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_necessary_columns(df, cols):\n",
    "    df_cleaned = df[cols]\n",
    "    return df_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(df):\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned.dropna(inplace=True)\n",
    "    return df_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The `handle_missing_values()` function is used to fill in missing values in the `FireCause` column before removing all rows with missing values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned['FireCause'].fillna('Undetermined', inplace=True)\n",
    "    df_cleaned = missing_values(df_cleaned)\n",
    "    return df_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicates:\n",
    "*We are using `UniqueFireIdentifier` column to identify duplicates. Unique identifier assigned to each wildland fire.  yyyy = calendar year, SSUUUU = POO protecting unit identifier (5 or 6 characters), xxxxxx = local incident identifier (6 to 10 characters)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicated_values(df):\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned.drop_duplicates(subset='UniqueFireIdentifier', keep='first', inplace=True)\n",
    "    return df_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Type Conversion:\n",
    "*The `convert_date()` function converts dates into a useable format for our API and handles `pandas.to_datetime` limitations and human errors when entering data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(dates):\n",
    "    for i in range(len(dates)):\n",
    "        try:\n",
    "            pd.to_datetime(dates[i]).date()\n",
    "        except:\n",
    "            dates[i] = np.nan\n",
    "        else: \n",
    "            dates[i] = pd.to_datetime(dates[i]).date()\n",
    "    return dates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The `calc_days()` function calculates the difference in days between two dates.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_days(end_dates, start_dates):\n",
    "    days = []\n",
    "    for i in range(len(end_dates)):\n",
    "        end_date = end_dates[i]\n",
    "        start_date = start_dates[i]\n",
    "        days.append((end_date - start_date).days)\n",
    "    return days"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The `data_conversion()` function converts our date columns to a usable format, converts `FireCause` column to a categorical column using `.map(cause_mapping)`, and adds `FireDuration` and `CausedByWeather` columns.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_conversion(df):\n",
    "    df_cleaned = df.copy()\n",
    "\n",
    "    # Convert FireDiscoveryDateTime and FireOutDateTime to useable format\n",
    "    df_cleaned['FireDiscoveryDateTime'] = convert_date(df_cleaned['FireDiscoveryDateTime'])\n",
    "    df_cleaned['FireOutDateTime'] = convert_date(df_cleaned['FireOutDateTime'])\n",
    "    df_cleaned = missing_values(df_cleaned) # Remove missing values after conversion\n",
    "\n",
    "    # Add FireDuration column\n",
    "    df_cleaned['FireDuration'] = calc_days(df_cleaned['FireOutDateTime'], df_cleaned['FireDiscoveryDateTime'])\n",
    "\n",
    "    # Convert FireCause to categorical column\n",
    "    cause_mapping = {'Human': 1, 'Natural': 2, 'Unknown': 3, 'Undetermined': 4, np.nan: 4}\n",
    "    df_cleaned['FireCause'] = df_cleaned['FireCause'].map(cause_mapping)\n",
    "\n",
    "    # Add CausedByWeather column\n",
    "    df_cleaned['CausedByWeather'] = df_cleaned['FireCause'].apply(lambda x: 1 if x == 2 else 0)\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting Outliers:\n",
    "*The `remove_outliers()` function is used to remove rows from a DataFrame based on a given boolean mask of outliers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, outliers):\n",
    "    for row in df.index:\n",
    "        if outliers[row] == True:\n",
    "            df.drop(row, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The `calculate_outliers()` function calculates outliers based on the IQR range. The IQR is less sensitive to extreme values and can handle non-normal distributions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_outliers(data):\n",
    "    outliers = []\n",
    "    Q1, Q3 = np.percentile(data, [25, 75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    if data < lower_bound or data > upper_bound:\n",
    "        outliers.append(True)\n",
    "    else:\n",
    "        outliers.append(False)\n",
    "    return outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The `detect_location_outliers()` function utilizes the geopy library and the Nominatim geocoder for reverse geocoding.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_locations(df):\n",
    "    geolocator = Nominatim(user_agent=\"Geolocation\")\n",
    "    outliers = []\n",
    "\n",
    "    for row in df.index:\n",
    "        county = df['POOCounty'][row].lower()\n",
    "        lat = df['InitialLatitude'][row]\n",
    "        long = df['InitialLongitude'][row]\n",
    "\n",
    "        location = geolocator.reverse((lat, long), exactly_one=True)\n",
    "        valid_county = location.raw.get('address', {}).get('county').lower()\n",
    "        valid_country = location.raw.get('address', {}).get('country').lower()\n",
    "\n",
    "        if location is None or county not in valid_county or valid_country != 'United States':\n",
    "            outliers.append(True)\n",
    "        else:\n",
    "            outliers.append(False)\n",
    "    return outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*And FINALLY - The `handle_outliers()` function that handles outliers in the given DataFrame.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(df):\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    fire_duration_outliers = calculate_outliers(df_cleaned['FireDuration'])  # Analyze fire duration values\n",
    "    df_cleaned = remove_outliers(df_cleaned, fire_duration_outliers)\n",
    "\n",
    "    fire_location_outliers = valid_locations(df_cleaned)  # Validate locations\n",
    "    df_cleaned = remove_outliers(df_cleaned, fire_location_outliers)\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_NAME)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = keep_necessary_columns(df, COLS)\n",
    "df.to_csv(COLS_RED_CSV, index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = missing_values(df)\n",
    "df.to_csv(MISS_CSV, index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = duplicated_values(df)\n",
    "df.to_csv(DUP_CSV, index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DUP_CSV)\n",
    "df = data_conversion(df)\n",
    "df.to_csv(DATA_CONV_CSV, index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = handle_outliers(df)\n",
    "df.to_csv(CSV_OUTLIERS, index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
