{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Forest Fire Prediction**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Data cleaning\n",
    "\n",
    "In this section, we will focus on cleaning the raw data to ensure its quality and consistency. Data cleaning is an essential step in the data preprocessing pipeline, as it helps to eliminate errors, handle missing values, and transform the data into a suitable format for analysis.\n",
    "\n",
    "# Handling Missing Values\n",
    "\n",
    "Missing values are a common occurrence in datasets and can pose challenges during analysis. Here are some techniques to handle missing values:\n",
    "\n",
    "- **Identify Missing Values**: Start by identifying missing values in the dataset using functions like `isnull()` or `isna()`.\n",
    "\n",
    "- **Drop Rows or Columns**: If the missing values are limited to a few rows or columns, you can choose to drop them using functions like `dropna()`.\n",
    "\n",
    "- **Imputation**: For missing values in numerical data, you can fill them with statistical measures like mean, median, or interpolation. For categorical data, you can fill missing values with the most frequent category.\n",
    "\n",
    "# Handling Duplicates\n",
    "\n",
    "Duplicates in the dataset can lead to biased results and skewed analysis. Here's how to handle duplicates:\n",
    "\n",
    "- **Identify Duplicates**: Use functions like `duplicated()` to identify duplicated rows in the dataset.\n",
    "\n",
    "- **Remove Duplicates**: If duplicates are found, you can remove them using `drop_duplicates()` function.\n",
    "\n",
    "# Data Type Conversion\n",
    "\n",
    "Sometimes, the data types of columns may need to be converted to a different format for analysis. Here are some common data type conversions:\n",
    "\n",
    "- **Numeric Conversion**: Convert columns from object or string types to numeric types(e.g., float or integer) using `astype()` or `to_numeric()`.\n",
    "\n",
    "- **Datetime Conversion**: Convert columns containing dates or timestamps to datetime format using `to_datetime()`.\n",
    "\n",
    "# Outliers Detection and Treatment\n",
    "\n",
    "Outliers can significantly impact statistical analysis and machine learning models. Consider the following steps for outlier detection and treatment:\n",
    "\n",
    "- **Visualize Data**: Plotting box plots or histograms can help identify potential outliers.\n",
    "\n",
    "- **Define Thresholds**: Establish thresholds or statistical measures(e.g., mean Â± 3 standard deviations) to determine outliers.\n",
    "\n",
    "- **Handling Outliers**: Decide on appropriate actions for outliers, such as removal, imputation, or transforming them to a more suitable value.\n",
    "\n",
    "These are just a few examples of common data cleaning tasks. The specific cleaning steps may vary depending on the nature of the dataset and the analysis goals. The goal of data cleaning is to ensure the accuracy, consistency, and reliability of the data for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note if running on a clean environment, need to install missing modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date as dt\n",
    "import re\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_NAME = 'fire_history.csv'\n",
    "CLEAN_COL_CSV = 'fire_history_clean_cols.csv'\n",
    "CLEAN_MISS_CSV = 'fire_history_clean_miss.csv'\n",
    "CLEAN_DUP_CSV = 'fire_history_dup.csv'\n",
    "DATA_CONV_CSV = 'fire_history_data_conv.csv'\n",
    "OUTLIERS_CSV = 'fire_history_outliers.csv'\n",
    "\n",
    "COLS = []\n",
    "\n",
    "COLS1 = ['UniqueFireIdentifier', 'FireDiscoveryDateTime', 'FireOutDateTime',\n",
    "        'POOCounty', 'InitialLatitude', 'InitialLongitude', 'FireCause']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unnecessary columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_necessary_columns(df, cols):\n",
    "    df_cleaned = df[cols]\n",
    "    return df_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Missing Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(df):\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned['FireCause'].fillna('Undetermined', inplace=True)\n",
    "    df_cleaned.dropna(inplace=True)\n",
    "    return df_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Duplicates: <br>\n",
    "<br>\n",
    "*We are using `UniqueFireIdentifier` column to identify duplicates. Unique identifier assigned to each wildland fire.  yyyy = calendar year, SSUUUU = POO protecting unit identifier (5 or 6 characters), xxxxxx = local incident identifier (6 to 10 characters)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_values(df):\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned.drop_duplicates(subset='UniqueFireIdentifier', keep='first', inplace=True)\n",
    "    # df_cleaned.drop_duplicates(keep='first', inplace=True)\n",
    "    return df_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Type Conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(dates):\n",
    "    for i in range(len(dates)):\n",
    "        dates[i] = pd.to_datetime(dates[i]).date()\n",
    "    return dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_days(end_dates, start_dates):\n",
    "    days = []\n",
    "    for i in range(len(end_dates)):\n",
    "        end_date = end_dates[i]\n",
    "        start_date = start_dates[i]\n",
    "        days.append((end_date - start_date).days)\n",
    "    return days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_type_conversion(df):\n",
    "    df_cleaned = df.copy()\n",
    "\n",
    "    # Convert FireDiscoveryDateTime and FireOutDateTime to useable format\n",
    "    df_cleaned['FireDiscoveryDateTime'] = convert_date(df_cleaned['FireDiscoveryDateTime'])\n",
    "    df_cleaned['FireOutDateTime'] = convert_date(df_cleaned['FireOutDateTime'])\n",
    "\n",
    "    # Add FireDuration column\n",
    "    df_cleaned['FireDuration'] = calc_days(df_cleaned['FireOutDateTime'], df_cleaned['FireDiscoveryDateTime'])\n",
    "\n",
    "    # Convert FireCause to categorical column\n",
    "    cause_mapping = {'Human': 1, 'Natural': 2, 'Unknown': 3, 'Undetermined': 4}\n",
    "    df_cleaned['FireCause'] = df_cleaned['FireCause'].map(cause_mapping)\n",
    "\n",
    "    # Add CausedByWeather column\n",
    "    df_cleaned['CausedByWeather'] = df_cleaned['FireCause'].apply(lambda x: 1 if x==2 else 0)\n",
    "\n",
    "    return df_cleaned\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers Detection and Treatment:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The `remove_outliers()` function is used to remove rows from a DataFrame based on a given boolean mask of outliers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, outliers):\n",
    "    return df[~outliers]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The `calculate_outliers()` function calculates outliers based on the IQR range. The IQR is less sensitive to extreme values and can handle non-normal distributions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_outliers(series):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return (series < lower_bound) | (series > upper_bound)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The `detect_outliers()` function utilizes the geopy library and the Nominatim geocoder for reverse geocoding. The function takes latitude and longitude values as input and iterates over each pair to reverse geocode and obtain the country information.*<br>\n",
    "*If the location is not within the USA, or if the reverse geocoding fails to provide country information, it is considered an outlier.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(latitude, longitude):\n",
    "    geolocator = Nominatim(user_agent=\"Geolocation\")\n",
    "    country = \"United States\"\n",
    "    outliers = []\n",
    "    for lat, long in zip(latitude, longitude):\n",
    "        location = geolocator.reverse((lat, long), exactly_one=True)\n",
    "        if location is None or location.raw.get('address', {}).get('country')!= country:\n",
    "            outliers.append(True)\n",
    "        else:\n",
    "            outliers.append(False)\n",
    "    return pd.Series(outliers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*And FINALLY - The `handle_outliers()` function handles outliers in the input DataFrame by analyzing fire duration outliers and Fire location outliers. It utilizes helper functions `calculate_outliers()` and `remove_outliers()` that we saw above for handling fire duration outliers based on IQR range, and a custom function `detect_outliers()` for detecting fire location outliers based on latitude and longitude.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(df):\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    # Analyze fire duration and location outliers\n",
    "    fire_duration_outliers = calculate_outliers(df_cleaned['FireDuration'])\n",
    "    df_cleaned = remove_outliers(df_cleaned, fire_duration_outliers)\n",
    "\n",
    "    fire_location_outliers = detect_outliers(df_cleaned['InitialLatitude'], df_cleaned['InitialLongitude'])\n",
    "    df_cleaned = remove_outliers(df_cleaned, fire_location_outliers)\n",
    "\n",
    "    return df_cleaned\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
